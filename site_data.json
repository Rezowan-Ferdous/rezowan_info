{
    "email": "shuvorezowan@gmail.com",
    "projects": [],
    "posts": [
        {
            "id": 1,
            "title": "Welcome to my new Blog",
            "slug": "welcome-blog",
            "content": "<p>This is the first post on my new blog system!</p>",
            "date_posted": "2026-02-09T00:19:36.096068"
        }
    ],
    "publications": [
        {
            "id": "j1",
            "title": "Deep Learning and Attention-Based Methods for Human Activity Recognition and Anticipation: A Review",
            "authors": "Rezowan Shuvo, et al.",
            "venue": "Cognitive Computation (Springer, Q1)",
            "year": 2025,
            "link": "https://link.springer.com/article/10.1007/s12559-025-10513-2",
            "code_url": "",
            "project_url": "",
            "pub_type": "Journal (Q1)",
            "abstract": "In recent years, there has been a significant increase in research focused on Human Activity Analysis (HAA). This field has progressed from basic activity recognition tasks to addressing more challenging ones, such as predicting future human actions based on partially observed videos and even predicting actions before they happen. The evolution of HAA has been driven by recent advancements in attention-based models like Transformers, along with a wide range of applications from security surveillance to advanced monitoring systems, behaviour analysis, and more. A comprehensive review of HAA literature from 2017 to 2025, with a novel taxonomy emphasising activity recognition, prediction, and anticipation, is presented. We critically review and examine recognition methods from trimmed and untrimmed videos, context-aware and trajectory-based prediction, and short-term and long-term anticipation. Through a comprehensive analysis, we review and evaluate key aspects of this domain, including attention-based contextual comprehension, temporal dynamics modelling, and multi-model fusion methods. Furthermore, we critically examine and assess the public datasets utilised in driving this research forward, pinpointing limitations and primary challenges within this domain. Finally, the paper provides a summary of recent developments in HAA and suggests future directions, with the hope that it will serve as a valuable reference for researchers in the field."
        },
        {
            "id": "j2",
            "title": "MSBATN: Multi-Stage Boundary-Aware Transformer Network for action segmentation in untrimmed surgical videos",
            "authors": "Rezowan Shuvo, MS Mekala, Eyad Elyan",
            "venue": "Computer Vision and Image Understanding (Elsevier, Q1)",
            "year": 2025,
            "link": "https://www.sciencedirect.com/science/article/abs/pii/S1077314225002449",
            "code_url": "",
            "project_url": "",
            "pub_type": "Journal (Q1)",
            "abstract": "Understanding actions within surgical workflows is critical for evaluating post-operative outcomes and enhancing surgical training and efficiency. Capturing and analysing long sequences of actions in surgical settings is challenging due to the inherent variability in individual surgeon approaches, which are shaped by their expertise and preferences. This variability complicates the identification and segmentation of distinct actions with ambiguous boundary start and end points. The traditional models, such as MS-TCN, which rely on large receptive fields, cause over-segmentation or under-segmentation, where distinct actions are incorrectly aligned. To address these challenges, we propose the Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention to improve action segmentation. Our approach effectively manages the complexity of varying action durations and subtle transitions by accurately identifying start and end action boundaries in untrimmed surgical videos. MSBATN introduces a novel unified loss function that optimises action classification and boundary detection as interconnected tasks. Unlike conventional binary boundary detection methods, our innovative boundary weighing mechanism leverages contextual information to precisely identify action boundaries. Extensive experiments on three challenging surgical datasets demonstrate that MSBATN achieves state-of-the-art performance, with superior F1 scores at 25% and 50% thresholds and competitive results across other metrics."
        },
        {
            "id": "j3",
            "title": "Deep Learning Models for the Diagnosis and Screening of COVID-19: A Systematic Review",
            "authors": "Rezowan Shuvo, et al.",
            "venue": "SN Computer Science (Springer, Q2)",
            "year": 2022,
            "link": "https://pubmed.ncbi.nlm.nih.gov/35911439/",
            "code_url": "",
            "project_url": "",
            "pub_type": "Journal",
            "abstract": "A systematic review of deep learning models used for COVID-19 diagnosis and screening from medical imaging."
        },
        {
            "id": "c1",
            "title": "GPAT: Goal-Progression-Aware Transformer with Recursive Grammar Induction for Action Anticipation",
            "authors": "Rezowan Shuvo, MS Mekala, Eyad Elyan",
            "venue": "European Conference on Computer Vision (ECCV) (Rank A*) (Submitted)",
            "year": 2024,
            "link": "#",
            "code_url": "",
            "project_url": "",
            "pub_type": "Conference",
            "abstract": " Anticipating future actions in videos is crucial for domains such as autonomous systems and surgical robotics. Transformer-based architectures have advanced the state of the art by capturing long-range temporal dependencies, but they remain vulnerable to dataset biases, struggle with long unstruc- tured contexts, and often function as ‚Äúblack boxes,‚Äù offering little interpretability. To address these challenges, we propose aunified neuro-symbolic framework, Goal-progress Aware Trans- former (GPAT), that integrates neural sequence modelling with structured priors. Our approach combines: (i) a self-supervised spatial feature extractor (DINOv3) with adaptive sampling and sparse attention to efficiently scale to long surgical and pro- cedural videos, (ii) a Goal-conditioned Multivariate Markov Chain (GcMMC) that models goal-conditioned and object-centric state transitions, and (iii) a Duration-Aware Recursive Grammar Induction (DARGI) that encodes temporal plausibility and task structure while constraining prediction to grammatically valid sequences. Extensive experiments on public datasets show that our method improves cross-dataset generalisation, handles rare action classes more reliably, and provides transparent reasoning through grammar-based parses and goal-conditioned transitions, enabling interpretable long-horizon action anticipation. We further analyse the effect of grammar size and rule complexity, showing that balanced grammars yield the best trade-off between coverage and robustness, while improved GcMMC significantly reduces implausible long-horizon predictions. Using public surgical and non-surgical datasets, experiments showed that GPAT achieves state-of-the-art performance, improving mean-over-class accuracy"
        },
        {
            "id": "c2",
            "title": "Chirality-Aware Grammar-Guided Surgical Action Anticipation from Video",
            "authors": "Rezowan Shuvo, MS Mekala, Eyad Elyan",
            "venue": "Human Robotic Interactions (HRI) (Rank A)",
            "year": 2026,
            "link": "#",
            "code_url": "",
            "project_url": "https://doi.org/10.1145/3757279.3785551",
            "pub_type": "Conference",
            "abstract": "Anticipating surgical actions requires more than recognising motion patterns, it also demands adherence to procedural logic and the resolution of subtle ambiguities, such as distinguishing mirrored grasp‚Äìretract interactions. However, existing Transformer-based models often fall short in this domain, often producing structurally invalid step sequences and misclassifying chirally opposite actions that appear visually similar. To address these limitations, we in- troduce a neuro-symbolic framework centred on a Probabilistic Temporal Grammar (PTG). The grammar was constructed from a unified corpus of surgical data (Ground-Truth), by capturing proce- dural structure, temporal priors, and chirality-aware terminals for opposite actions (e.g., ùëùùë¢ùë†‚Ñé_ùëõùëíùëíùëëùëôùëí ‚Üî ùëùùë¢ùëôùëô_ùë†ùë¢ùë°ùë¢ùëüùëí) directly into its rules. To enforce causal consistency, the PTG incorporates a Goal-conditioned Multivariate Markov Chain (GcMMC) that mod- els evolving object-action dependencies. Our framework employs a two-stage process: a V-JEPA-powered Transformer generates raw forecasts of future actions and durations, which are then refined by a constrained parsing algorithm guided by the PTG. Candidate fu- tures are jointly scored for structural validity, temporal plausibility, and causal grounding. By explicitly encoding surgical logic into a unified neuro-symbolic system. Experiments across three publicly available surgical datasets show that our approach outperforms state-of-the-art anticipation models. Importantly, by generating interpretable and procedurally consistent forecasts of upcoming actions, PTG establishes the predictive foundation required for proactive robotic assistance and safe human‚Äìrobot collaboration in the operating room."
        },
        {
            "id": "c3",
            "title": "Performance Analysis of Different Loss Function in Face Detection Architectures",
            "authors": "Rezowan Hossain Ferdous, et al.",
            "venue": "International Conference on Trends in Computational and Cognitive Engineering, Dhaka",
            "year": 2020,
            "link": "#",
            "code_url": "",
            "project_url": "",
            "pub_type": "Conference",
            "abstract": ""
        }
    ],
    "experience": [
        {
            "id": 1,
            "position": "PhD Candidate and Teaching Assistant",
            "company": "Robert Gordon University",
            "location": "Aberdeen, Scotland",
            "start_date": "2023-03-01",
            "end_date": null,
            "description": "Conducting advanced research on complex surgical video understanding, focusing on spatial-temporal context analysis. Developing frameworks for future action prediction and safety evaluation to detect potential surgical errors. Implementing novel architectures using Vision Transformers and State Space Models (SSM) for segmentation and detection tasks. Pioneering the application of Large Language Models (LLMs) and Vision-Language Models (VLMs) to enhance visual reasoning for complex robotic tasks. Teaching students; conducted labs on Python programming, Data Science, and NLP."
        },
        {
            "id": 2,
            "position": "Software Developer",
            "company": "Beraten Software Corporation",
            "location": "Portland, OR-USA (Remote)",
            "start_date": "2022-05-01",
            "end_date": "2023-03-01",
            "description": "Engineered software solutions for Indian Child Welfare and Tribal Court systems using .NET Core 6 Framework and MSSQL Server. Developed and maintained e-court systems for criminal case management, ensuring robust data handling and security. Managed project version control and collaboration using Bitbucket and GitHub. Integrated complex database schemas with .NET backend services to support tribal healthcare and welfare services."
        },
        {
            "id": 3,
            "position": "Research Assistant",
            "company": "Time Research and Innovation (TRI)",
            "location": "Southampton, UK (Dhaka Branch)",
            "start_date": "2020-10-01",
            "end_date": "2022-10-01",
            "description": " Conducted research projects and led the Software Development Life Cycle (SDLC) in collaboration with developers. Assisted in system designing and requirement engineering to align technical outputs with research goals. Developed and deployed an end-to-end facial recognition system for smart attendance with MLOps. Analyzed chest X-ray data for COVID-19 detection using deep learning models."
        }
    ],
    "skills": {
        "Programming": [
            "Python",
            "C# (.NET Core)",
            "Bash",
            "SQL"
        ],
        "MLOps Tools": [
            "Docker",
            "CI/CD",
            "Git",
            "Packaging",
            "Pytest",
            "Bitbucket"
        ],
        "Tools & Platforms": [
            "AWS",
            "S3",
            "EC2",
            "Raspberry Pi",
            "Overleaf",
            "MSSQL Server",
            "MATLAB",
            "ROS",
            "Arduino"
        ],
        "Deep Learning & ML": [
            "PyTorch",
            "TensorFlow",
            "Keras",
            "Hugging Face",
            "Scikit-learn",
            "Pandas",
            "NumPy",
            "SciPy",
            "Matplotlib",
            "Seaborn"
        ],
        "Mathematics": [
            "Linear Algebra",
            "Calculus",
            "Probability",
            "Statistics"
        ],
        "AI & Analytics": [
            "Data Analytics",
            "Machine Learning (ML)",
            "Deep Learning (DL)",
            "AI",
            "Classification",
            "Regression",
            "Object Detection",
            "Predictive Analysis"
        ],
        "Soft Skills": [
            "Project Management",
            "Business & Data-Driven Decision Making",
            "Communication",
            "Team Collaboration"
        ],
        "Research Interests": [
            "Predictive Analysis",
            "Robotics (Vision)",
            "Action Analysis",
            "Video Analysis",
            "Inspections",
            "Reasoning"
        ]
    },
    "awards": [
        {
            "title": "PhD Studentship",
            "year": "2023",
            "organization": "Robert Gordon University",
            "description": "Fully funded PhD scholarship awarded by the School of Computing Engineering and Technology."
        },
        {
            "title": "Employee of the Month",
            "year": "2021",
            "organization": "Time Research and Innovation (TRI)",
            "description": "Recognized for outstanding performance and contributions to research development."
        }
    ],
    "volunteering": [
        {
            "role": "Reviewer",
            "period": "Jun 2024 \u2013 Present",
            "organization": "IEEE Transactions on Neural Networks and Learning Systems",
            "description": "Reviewing technical manuscripts and providing constructive feedback."
        },
        {
            "role": "Volunteer",
            "period": "May 2025 \u2013 Present",
            "organization": "Active School Aberdeen, Scotland",
            "description": "Engaging in voluntary sessions to support school activities."
        }
    ],
    "references": [
        {
            "name": "Eyad Elyan",
            "title": "Professor, Robert Gordon University",
            "email": "e.elyan@rgu.ac.uk"
        },
        {
            "name": "Md Junayed Hasan",
            "title": "AI Engineer, Subsea7",
            "email": "junayed.hasan@subsea7.com"
        },
        {
            "name": "Dr. M Shamim Kaiser",
            "title": "Professor, Jahangirnagar University, Dhaka",
            "email": "mskaiser@juniv.edu"
        }
    ]
}